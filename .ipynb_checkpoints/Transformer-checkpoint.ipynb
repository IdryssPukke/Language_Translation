{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b725cd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Uzytkowe\\Anaconda\\envs\\Studies\\lib\\site-packages\\torchaudio\\backend\\utils.py:66: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import Levenshtein\n",
    "import openai\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "import torchtext.datasets as datasets\n",
    "import spacy\n",
    "import GPUtil\n",
    "import warnings\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from transformers import MarianMTModel, MarianTokenizer, GPT2Tokenizer, AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "from nltk.tokenize import sent_tokenize, LineTokenizer\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "from pathlib import Path\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "from Levenshtein import distance\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from translate.storage.tmx import tmxfile\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from bayes_opt import BayesianOptimization, UtilityFunction\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Set to False to skip notebook execution (e.g. for debugging)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RUN_EXAMPLES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e83f9c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Wie geht es dir?'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#szybkie sprawdzenie czy model działa\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-de\"\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)\n",
    "translator(\"How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "412ba579",
   "metadata": {},
   "outputs": [],
   "source": [
    "#przygotowanie plików .txt z korpusu równoległego .tmx\n",
    "\n",
    "with open(\"de-en.tmx\", 'rb') as fin:\n",
    "    tmx_file = tmxfile(fin, 'en', 'de')\n",
    "file_de = open(\"de_origin/de.txt\",'w', encoding=\"utf-8\")\n",
    "file_en = open(\"en_origin/en.txt\",'w', encoding=\"utf-8\")\n",
    "\n",
    "for node in tmx_file.unit_iter():\n",
    "    file_en.write(node.target + '\\n')\n",
    "    file_de.write(node.source + '\\n')\n",
    "    # print(node.target, node.source)\n",
    "    \n",
    "file_de.close()\n",
    "file_en.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb43a92e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ANNUAL ACTIVITY REPORT 2007 EUROPEAN COURT OF AUDITORS \\n', 'Luxembourg: Office for Official Publications of the European Communities, 2008 ISBN 978-92-9207-005-2 © European Communities, 2008 Reproduction is authorised provided the source is acknowledged. \\n', '3 4–5 6–7 8–13 14–17 18–25 26–27 28–29 30–35 36–37 38–43 44–45 PRESIDENT’S FOREWORD MISSION, VISION, VALUES AND STRATEGIC OBJECTIVES THE COURT’S ROLE AND WORK GOVERNANCE AND ORGANISATION OVERVIEW OF AUDIT REPORTS AND OPINIONS FOLLOW-UP AND IMPACT THE COURT’S VIEW THE COURT’S WORK IN 2007 AND BEYOND INTERNATIONAL COOPERATION HUMAN RESOURCES FINANCIAL INFORMATION CONTENTS \\n', 'PRESIDENT’S FOREWORD 4 Bringing the European Court of Auditors closer to EU citizens is one of our key objectives and part of our mission to promote transparency and accountability. \\n', 'I, therefore, take great pleasure in welcoming you to the first annual activity report of the European Court of Auditors. \\n', 'It provides an overview of the Court and an account of its activities in 2007, a year in which it celebrated its 30th anniversary as the external auditor of the EU dedicated to contributing to improving financial management and acting as the independent guardian of the financial interests of its citizens. \\n', 'The main contribution the Court makes is through its audits and reports which help the auditee to improve their financial management and which assist the Discharge Authority (the European Parliament and Council) in overseeing the implementation of the EU budget. \\n', 'This report gives an overview of audit reports published in 2007. It highlights the main conclusions issued on the implementation of the EU budget 2006 and on the sound financial management of EU funds. \\n', 'The Court not only reports on past financial management, it also actively contributes to building the EU’s financial control framework. \\n', 'The year 2007 was a significant one for the management of EU funds. \\n']\n",
      "['JÄHRLICHER TÄTIGKEITSBERICHT 2007 EUROPÄISCHER RECHNUNGSHOF \\n', 'Luxemburg: Amt für amtliche Veröffentlichungen der Europäischen Gemeinschaften, 2008 ISBN 978-92-9207-003-8 © Europäische Gemeinschaften, 2008 Nachdruck mit Quellenangabe gestattet. \\n', '3 4–5 6–7 8–13 14–17 18–25 26–27 28–29 30–35 36–37 38–43 44–45 VORWORT DES PRÄSIDENTEN AUFTRAG, LEITBILD, WERTE UND STRATEGISCHE ZIELE ROLLE UND ARBEIT DES HOFES LEITUNGSSTRUKTUR UND ORGANISATION ÜBERSICHT ÜBER DIE PRÜFUNGSBERICHTE UND STELLUNGNAHMEN DES HOFES WEITERVERFOLGUNG UND WIRKUNG DER STANDPUNKT DES HOFES DIE ARBEIT DES HOFES IM JAHR 2007 UND DARÜBER HINAUS INTERNATIONALE ZUSAMMENARBEIT PERSONAL FINANZINFORMATIONEN INHALT \\n', 'VORWORT DES PRÄSIDENTEN 4 Eines der wichtigsten Ziele des Hofes und Teil seines Auftrags im Hinblick auf die Förderung von Transparenz und Rechenschaftspflicht besteht darin, den Europäischen Rechnungshof den EU-Bürgern näherzubringen. \\n', 'Es ist mir daher eine große Freude, Sie als Leser des ersten Jährlichen Tätigkeitsberichts des Europäischen Rechnungshofs begrüßen zu dürfen. \\n', 'Mit dem Bericht soll ein Gesamtüberblick über den Hof vermittelt und Rechenschaft über seine Tätigkeiten im Jahr 2007 abgelegt werden, dem Jahr, in dem er sein 30-jähriges Bestehen als externer Prüfer der Europäischen Union feiert, der laut seinem Auftrag zur Verbesserung des EU-Finanzmanagements beiträgt und als unabhängiger Hüter der finanziellen Interessen der Unionsbürger fungiert. \\n', 'Der Hof leistet seinen Hauptbeitrag über seine Prüfungen und Berichte, die es der geprüften Stelle ermöglichen, ihr Finanzmanagement zu verbessern, und die Entlastungsbehörde (Europäisches Parlament und Rat) bei ihrer Kontrolle der Ausführung des EU-Haushalts unterstützen. \\n', 'Der vorliegende Bericht gibt einen Überblick über die im Jahr 2007 veröffentlichten Prüfungsberichte und beleuchtet die wichtigsten Schlussfolgerungen in Bezug auf die Ausführung des EU-Haushaltsplans 2006 und die Wirtschaftlichkeit der Haushaltsführung der EU-Mittel. \\n', 'Der Hof berichtet nicht nur über das Finanzmanagement vergangener Jahre, sondern wirkt auch aktiv am Aufbau eines Finanzkontrollrahmens der EU mit. \\n', 'Das Jahr 2007 war für das EU-Finanzmanagement von großer Bedeutung. \\n']\n"
     ]
    }
   ],
   "source": [
    "#sprawdzenie czy pliki zostały odpowiednio utworzone\n",
    "\n",
    "with open(\"en_origin/en.txt\", encoding=\"utf-8\") as myfile:\n",
    "    head = [next(myfile) for x in range(3)]\n",
    "print(head)\n",
    "\n",
    "with open(\"de_origin/de.txt\", encoding=\"utf-8\") as myfile:\n",
    "    head = [next(myfile) for x in range(3)]\n",
    "print(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "540e01ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ograniczenie wielkości dostępnego korpusu do 10 tys. zdań w celu przyśpieszenia obliczeń.\n",
    "\n",
    "with open(\"de-en.tmx\", 'rb') as fin:\n",
    "    tmx_file = tmxfile(fin, 'en', 'de')\n",
    "file_de = open(\"de_origin/de_10000.txt\",'w', encoding=\"utf-8\")\n",
    "\n",
    "i = 0\n",
    "for node in tmx_file.unit_iter():\n",
    "    if(i<10000):\n",
    "        file_de.write(node.source + '\\n')\n",
    "    i+=1\n",
    "    \n",
    "file_de.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbc90ef0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.generate(**model_inputs) no_params\n",
      "File exists: Translation\\Auto_no_params.txt \n",
      "\n",
      "model.generate(**model_inputs, do_sample=True, top_k=0) sample\n",
      "File exists: Translation\\Auto_sample.txt \n",
      "\n",
      "model.generate(**model_inputs, do_sample=True, top_k=0, temperature=1) temperature\n",
      "File exists: Translation\\Auto_temperature.txt \n",
      "\n",
      "model.generate(**model_inputs, do_sample=True, top_k=50) top_k50\n",
      "File exists: Translation\\Auto_top_k50.txt \n",
      "\n",
      "model.generate(**model_inputs, do_sample=True, top_k=6) top_k6\n",
      "File exists: Translation\\Auto_top_k6.txt \n",
      "\n",
      "model.generate(**model_inputs, do_sample=True, top_k=6, top_p=0.40) top_p40\n",
      "File exists: Translation\\Auto_top_p40.txt \n",
      "\n",
      "model.generate(**model_inputs, do_sample=True, top_k=6, top_p=0.92) top_p92\n",
      "File exists: Translation\\Auto_top_p92.txt \n",
      "\n",
      "model.generate(**model_inputs, do_sample=True, top_k=0, repetition_penalty=0.5) repetition_penalty05\n",
      "File exists: Translation\\Auto_repetition_penalty05.txt \n",
      "\n",
      "model.generate(**model_inputs, do_sample=True, top_k=6, top_p=0.92, repetition_penalty=0.5) repetition_penalty_top_k6\n",
      "File exists: Translation\\Auto_repetition_penalty_top_k6.txt \n",
      "\n"
     ]
    }
   ],
   "source": [
    "###Direct translation to English\n",
    "\n",
    "#przygotowanie modelu \n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-de-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    " \n",
    "#zainicjowanie CUDA\n",
    "    \n",
    "dev = \"cuda:0\"\n",
    "device = torch.device(dev)\n",
    "model.to(device)\n",
    "\n",
    "#odpowiednio przygotowane parametry modeli\n",
    "args = [\n",
    "        'model.generate(**model_inputs)', \n",
    "        'model.generate(**model_inputs, do_sample=True, top_k=0)',\n",
    "        'model.generate(**model_inputs, do_sample=True, top_k=0, temperature=1)',\n",
    "        'model.generate(**model_inputs, do_sample=True, top_k=50)',\n",
    "        'model.generate(**model_inputs, do_sample=True, top_k=6)',\n",
    "        'model.generate(**model_inputs, do_sample=True, top_k=6, top_p=0.40)',\n",
    "        'model.generate(**model_inputs, do_sample=True, top_k=6, top_p=0.92)',\n",
    "        'model.generate(**model_inputs, do_sample=True, top_k=0, repetition_penalty=0.5)',\n",
    "        'model.generate(**model_inputs, do_sample=True, top_k=6, top_p=0.92, repetition_penalty=0.5)'\n",
    "       ]\n",
    "\n",
    "#nazwy plików zapisowych\n",
    "titles = [\n",
    "          'no_params',\n",
    "          'sample',\n",
    "          'temperature', \n",
    "          'top_k50',\n",
    "          'top_k6',\n",
    "          'top_p40',\n",
    "          'top_p92',\n",
    "          'repetition_penalty05',\n",
    "          'repetition_penalty_top_k6'\n",
    "         ]\n",
    "\n",
    "for arg,title in zip(args,titles):\n",
    "    print(arg, title)\n",
    "\n",
    "    if Path(\"Translation\\Auto_\"+title+\".txt\").is_file():\n",
    "        print(\"File exists: \" + \"Translation\\Auto_\"+title+\".txt \\n\")\n",
    "    else:\n",
    "        print(\"No file: \" + \"Translation\\Auto_\"+title+\".txt\")\n",
    "        file = open(\"Translation\\Auto_\"+title+\".txt\",'w', encoding=\"utf-8\")\n",
    "        with open(\"de-en.tmx\", 'rb') as fin:\n",
    "            tmx_file = tmxfile(fin, 'en', 'de')\n",
    "\n",
    "        i = 0\n",
    "        for node in tmx_file.unit_iter():\n",
    "            if i == 10000:\n",
    "                break\n",
    "\n",
    "            model_inputs = tokenizer(node.source, return_tensors=\"pt\", padding=True, truncation=True, max_length=500).to(device)\n",
    "            translated = eval(arg)\n",
    "            for t in translated:\n",
    "                if(i%100==0):\n",
    "                    print(repr(i) + \" \" + tokenizer.decode(t, skip_special_tokens=True) + '\\n')\n",
    "                file.write(tokenizer.decode(t, skip_special_tokens=True) + '\\n') \n",
    "            i+=1\n",
    "\n",
    "\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "975fe51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto_no_params.txt: BLEU = 0.7150162205110255\n",
      "Auto_repetition_penalty05.txt: BLEU = 0.6606883748361918\n",
      "Auto_repetition_penalty_top_k6.txt: BLEU = 0.6843873512995613\n",
      "Auto_sample.txt: BLEU = 0.6916898352809548\n",
      "Auto_temperature.txt: BLEU = 0.6901345568236875\n",
      "Auto_top_k50.txt: BLEU = 0.7016499119886753\n",
      "Auto_top_k6.txt: BLEU = 0.7068425277875855\n",
      "Auto_top_p40.txt: BLEU = 0.6813655959475835\n",
      "Auto_top_p92.txt: BLEU = 0.7056663074717361\n"
     ]
    }
   ],
   "source": [
    "###BLEU for Single language translation\n",
    "\n",
    "total = 10000\n",
    "\n",
    "file_org = open(\"En_origin\\en.txt\",'r+', encoding=\"utf-8\")\n",
    "file_eu = open(\"En_origin\\en_EU.txt\",'r+', encoding=\"utf-8\")\n",
    "file_google = open(\"En_origin\\en_google.txt\",'r+', encoding=\"utf-8\")\n",
    "\n",
    "reference1 = [] \n",
    "reference2 = []\n",
    "reference3 = []\n",
    "\n",
    "for i in range (total):\n",
    "    reference1.append(file_org.readline().split())\n",
    "    reference2.append(file_eu.readline().split())\n",
    "    reference3.append(file_google.readline().split())\n",
    "\n",
    "for x,filename in enumerate(os.listdir(Path(os.getcwd() + \"\\Translation\"))):\n",
    "    with open(Path(os.getcwd() + \"\\Translation\\\\\"+filename), 'r+', encoding=\"utf-8\") as results:\n",
    "        score = 0\n",
    "        for i in range (total):\n",
    "            translated = (results.readline().split())\n",
    "            score += sentence_bleu([reference1[i],reference2[i],reference3[i]], translated)\n",
    "        print(filename + \": BLEU = \" + repr(score/total))\n",
    "    results.close()\n",
    "file_org.close()\n",
    "file_eu.close()\n",
    "file_google.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b7e10fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: TranslationTwoLanguages\\Marian_no_params.txt \n",
      "\n",
      "File exists: TranslationTwoLanguages\\Marian_repetition_penalty05.txt \n",
      "\n",
      "File exists: TranslationTwoLanguages\\Marian_repetition_penalty_top_k6.txt \n",
      "\n"
     ]
    }
   ],
   "source": [
    "###Two languages translation\n",
    "\n",
    "model_checkpoint_pl = \"Helsinki-NLP/opus-mt-de-pl\"\n",
    "model_checkpoint_en = \"Helsinki-NLP/opus-mt-pl-en\"\n",
    "\n",
    "tokenizerPl = AutoTokenizer.from_pretrained(model_checkpoint_pl)\n",
    "modelPl = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint_pl)\n",
    "\n",
    "tokenizerEn = AutoTokenizer.from_pretrained(model_checkpoint_en)\n",
    "modelEn = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint_en)\n",
    " \n",
    "dev = \"cuda:0\"\n",
    "device = torch.device(dev)\n",
    "modelPl.to(device)\n",
    "modelEn.to(device)\n",
    "\n",
    "argsPl = [\n",
    "        'modelPl.generate(**model_inputs)', \n",
    "#        'modelPl.generate(**model_inputs, do_sample=True, top_k=0)',\n",
    "#        'modelPl.generate(**model_inputs, do_sample=True, top_k=0, temperature=1)',\n",
    "#        'modelPl.generate(**model_inputs, do_sample=True, top_k=50)',\n",
    "#        'modelPl.generate(**model_inputs, do_sample=True, top_k=6)',\n",
    "#        'modelPl.generate(**model_inputs, do_sample=True, top_k=6, top_p=0.40)',\n",
    "#        'modelPl.generate(**model_inputs, do_sample=True, top_k=6, top_p=0.92)',\n",
    "        'modelPl.generate(**model_inputs, do_sample=True, top_k=0, repetition_penalty=0.5)',\n",
    "        'modelPl.generate(**model_inputs, do_sample=True, top_k=6, top_p=0.92, repetition_penalty=0.5)'\n",
    "       ]\n",
    "argsEn = [\n",
    "        'modelEn.generate(**model_inputs)', \n",
    "#        'modelEn.generate(**model_inputs, do_sample=True, top_k=0)',\n",
    "#        'modelEn.generate(**model_inputs, do_sample=True, top_k=0, temperature=1)',\n",
    "#        'modelEn.generate(**model_inputs, do_sample=True, top_k=50)',\n",
    "#        'modelEn.generate(**model_inputs, do_sample=True, top_k=6)',\n",
    "#        'modelEn.generate(**model_inputs, do_sample=True, top_k=6, top_p=0.40)',\n",
    "#        'modelEn.generate(**model_inputs, do_sample=True, top_k=6, top_p=0.92)',\n",
    "        'modelEn.generate(**model_inputs, do_sample=True, top_k=0, repetition_penalty=0.5)',\n",
    "        'modelEn.generate(**model_inputs, do_sample=True, top_k=6, top_p=0.92, repetition_penalty=0.5)'\n",
    "       ]\n",
    "titles = [\n",
    "          'no_params',\n",
    "#          'sample',\n",
    "#          'temperature', \n",
    "#          'top_k50',\n",
    "#          'top_k6',\n",
    "#          'top_p40',\n",
    "#          'top_p92',\n",
    "          'repetition_penalty05',\n",
    "          'repetition_penalty_top_k6'\n",
    "         ]\n",
    "\n",
    "\n",
    "for i in range(len(titles)):\n",
    "    \n",
    "    if Path(\"TranslationTwoLanguages\\Marian_\"+titles[i]+\".txt\").is_file():\n",
    "        print(\"File exists: \" + \"TranslationTwoLanguages\\Marian_\"+titles[i]+\".txt \\n\")\n",
    "    else:\n",
    "        print(argsPl[i], titles[i])\n",
    "\n",
    "        file = open(\"TranslationTwoLanguages\\Marian_\"+titles[i]+\".txt\",'w', encoding=\"utf-8\")\n",
    "        with open(\"de-en.tmx\", 'rb') as fin:\n",
    "            tmx_file = tmxfile(fin, 'en', 'de')\n",
    "            \n",
    "        x = 0\n",
    "        for node in tmx_file.unit_iter():\n",
    "            if x == 10000:\n",
    "                break\n",
    "\n",
    "            model_inputs = tokenizerPl(node.source, return_tensors=\"pt\", padding=True, truncation=True, max_length=500).to(device)\n",
    "            translatedPl = eval(argsPl[i])\n",
    "\n",
    "            textPl = ''\n",
    "            for t in translatedPl:\n",
    "                textPl += tokenizerPl.decode(t, skip_special_tokens=True)\n",
    "\n",
    "            model_inputs = tokenizerEn(textPl, return_tensors=\"pt\", padding=True, truncation=True, max_length=500).to(device)\n",
    "            translatedEn = eval(argsEn[i])\n",
    "\n",
    "            for t in translatedEn:\n",
    "                file.write(tokenizerEn.decode(t, skip_special_tokens=True) + '\\n') \n",
    "                if(x%100==0):\n",
    "                    print(repr(x) + \" \" + tokenizerEn.decode(t, skip_special_tokens=True) + '\\n')\n",
    "            x+=1\n",
    "\n",
    "\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8aaf2d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marian_no_params.txt: BLEU = 0.5095654999135777\n",
      "Marian_repetition_penalty05.txt: BLEU = 0.46046313477259965\n",
      "Marian_repetition_penalty_top_k6.txt: BLEU = 0.47982806203191647\n"
     ]
    }
   ],
   "source": [
    "###BLEU for Two languages translation\n",
    "total = 10000\n",
    "\n",
    "\n",
    "\n",
    "file_org = open(\"En_origin\\en.txt\",'r+', encoding=\"utf-8\")\n",
    "file_eu = open(\"En_origin\\en_EU.txt\",'r+', encoding=\"utf-8\")\n",
    "file_google = open(\"En_origin\\en_google.txt\",'r+', encoding=\"utf-8\")\n",
    "\n",
    "reference1 = [] \n",
    "reference2 = []\n",
    "reference3 = []\n",
    "\n",
    "for i in range (total):\n",
    "    reference1.append(file_org.readline().split())\n",
    "    reference2.append(file_eu.readline().split())\n",
    "    reference3.append(file_google.readline().split())\n",
    "\n",
    "for x,filename in enumerate(os.listdir(Path(os.getcwd() + \"\\TranslationTwoLanguages\"))):\n",
    "    with open(Path(os.getcwd() + \"\\TranslationTwoLanguages\\\\\"+filename), 'r+', encoding=\"utf-8\") as results:\n",
    "        score = 0\n",
    "        for i in range (total):\n",
    "            translated = (results.readline().split())\n",
    "            score += sentence_bleu([reference1[i],reference2[i],reference3[i]], translated)\n",
    "\n",
    "        print(filename + \": BLEU = \" + repr(score/total))\n",
    "\n",
    "    results.close()\n",
    "file_org.close()\n",
    "file_eu.close()\n",
    "file_google.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca59c7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   top_k   |\n",
      "-------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.7434  \u001b[0m | \u001b[0m 48.38   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.7362  \u001b[0m | \u001b[0m 27.81   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.7326  \u001b[0m | \u001b[0m 48.66   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.7304  \u001b[0m | \u001b[0m 36.03   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.7409  \u001b[0m | \u001b[0m 35.19   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.7251  \u001b[0m | \u001b[0m 48.76   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.732   \u001b[0m | \u001b[0m 48.37   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.7252  \u001b[0m | \u001b[0m 42.2    \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.7334  \u001b[0m | \u001b[0m 35.54   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.737   \u001b[0m | \u001b[0m 20.67   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.7408  \u001b[0m | \u001b[0m 44.9    \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.7398  \u001b[0m | \u001b[0m 42.62   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.7244  \u001b[0m | \u001b[0m 35.3    \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.7398  \u001b[0m | \u001b[0m 26.81   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.7201  \u001b[0m | \u001b[0m 18.7    \u001b[0m |\n",
      "=====================================\n",
      "{'target': 0.7434057420737724, 'params': {'top_k': 48.38446211167016}}\n"
     ]
    }
   ],
   "source": [
    "## https://github.com/fmfn/BayesianOptimization\n",
    "#BayesianOptimization\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-de-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    " \n",
    "dev = \"cuda:0\"\n",
    "device = torch.device(dev)\n",
    "model.to(device)\n",
    "\n",
    "total = 100\n",
    "\n",
    "file_org = open(\"En_origin\\en.txt\",'r+', encoding=\"utf-8\")\n",
    "file_eu = open(\"En_origin\\en_EU.txt\",'r+', encoding=\"utf-8\")\n",
    "file_google = open(\"En_origin\\en_google.txt\",'r+', encoding=\"utf-8\")\n",
    "file_de = open(\"De_origin/de_10000.txt\", 'r+', encoding=\"utf-8\")\n",
    "\n",
    "to_translate = []\n",
    "reference1 = []\n",
    "reference2 = []\n",
    "reference3 = []\n",
    "\n",
    "for i in range (total):\n",
    "    to_translate.append(file_de.readline().split(\"\\n\")[0])\n",
    "    reference1.append(file_org.readline().split())\n",
    "    reference2.append(file_eu.readline().split())\n",
    "    reference3.append(file_google.readline().split())\n",
    "    \n",
    "def black_box_function(top_p=1,top_k=6,repetition_penalty=1):\n",
    "    score = 0\n",
    "    for i in range (total):\n",
    "        model_inputs = tokenizer(to_translate[i], return_tensors=\"pt\", padding=True, truncation=True, max_length=500).to(device)\n",
    "        translated = model.generate(**model_inputs, do_sample=True, top_k=math.floor(top_k), top_p=top_p,repetition_penalty=repetition_penalty)\n",
    "        for t in translated:\n",
    "            translated = tokenizer.decode(t, skip_special_tokens=True).split()\n",
    "        score += sentence_bleu([reference1[i],reference2[i],reference3[i]], translated)\n",
    "    return(score/total)\n",
    "\n",
    "\n",
    "pbounds = {#\"top_p\": [0.1, 1.0],\n",
    "            \"top_k\": [1, 50],\n",
    "          #  \"repetition_penalty\": [0.1, 1.0],\n",
    "          #  \"temperature\": [0.1, 1.0]\n",
    "          }\n",
    "\n",
    "optimizer = BayesianOptimization(f = black_box_function,\n",
    "                                     pbounds = pbounds, verbose = 2,\n",
    "                                     random_state = 4)\n",
    "\n",
    "\n",
    "optimizer.maximize(init_points = 5, n_iter = 10)\n",
    "\n",
    "print(optimizer.max)\n",
    "\n",
    "file_de.close()\n",
    "file_org.close()\n",
    "file_eu.close()\n",
    "file_google.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbf5d24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
